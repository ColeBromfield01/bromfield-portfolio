{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73Qw3cBdwRae"
      },
      "source": [
        "# Problem 1\n",
        "\n",
        "Just as $P(H | E_{1}) = \\frac{P(E_{1} | H)P(H)}{P(E_{1})}$:\n",
        "Our more informed posterior can take the form:\n",
        "\n",
        "$P(H | E_{1}, E_{2}) = \\frac{P(E_{2} | H, E_{1})P(H | E_{1})}{P(E_{2} | E_{1})}$\n",
        "\n",
        "Since $E_{1}$ and $E_{2}$ are independent,\n",
        "\n",
        "$P(E_{2}|H, E_{1}) = P(E_{2}|H)$\n",
        "\n",
        "and\n",
        "\n",
        "$P(E_{2}|E_{1}) = P(E_{2})$\n",
        "\n",
        "Therefore, our posterior can be rewritten as:\n",
        "\n",
        "$P(H | E_{1}, E_{2}) = \\frac{P(E_{2} | H)P(H | E_{1})}{P(E_{2})}$\n",
        "\n",
        "And thus, our X and Y values are:\n",
        "\n",
        "$X = P(E_{2}|H)$\n",
        "\n",
        "$Y = P(E_{2})$\n",
        "\n",
        "*Time spent: ~20 minutes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1T_I61WwRaf"
      },
      "source": [
        "# Problem 2\n",
        "## Part A\n",
        "### [1,1]\n",
        "With these parameters, we get a uniform distribution.  In a practical sense, we get an expected value for the probability of heads, but our confidence level is low, because all probabilities have equal likelihood.\n",
        "\n",
        "### [2,2]\n",
        "Once again, we get a symmetrical distribution, but no longer are all probabilities equally likely.  Now, more balanced values (closer to 50-50) have a higher likelihood, while fringe values (very high or very low probability of success) are less likely.\n",
        "\n",
        "### [10,10]\n",
        "The distribution is again centered, the strength of belief further increases.  Now, probabilities with any semblance of imbalance (outside of the 0.4-0.6 range) are highly unlikely.\n",
        "\n",
        "### [2,10]\n",
        "We now see a skewed distribution, with a low expected probability--probabilities above 0.5 approach a non-zero likelihood (suggesting a high strength of belief).\n",
        "\n",
        "### [10,2]\n",
        "This is simply the inverse of the [2,10] distribution--the expected probability is high, and probabilities below 0.5 are highly unlikely.\n",
        "\n",
        "## Part B\n",
        "I already knew the formula: divide alpha (the first parameter) by the sum of alpha and beta (the second parameter).  For example, for beta(10, 2), the expected probability is 10/(10+2) = 5/6.  Any pair of values with the same ratio will therefore produce the same expected probability, only differing in strength of belief (variance).\n",
        "\n",
        "*Time spent: ~ 15 minutes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6o3LaG0wRag"
      },
      "source": [
        "# Problem 3\n",
        "## Part A\n",
        "If we use weights of 3 and a bias term of -4, the output will be correct:\n",
        "* $y = $&sigma;$(3x_{1} + 3x_{2} - 4)$\n",
        "\n",
        "If $x_{1}$ and $x_{2}$ are both 0:\n",
        "\n",
        "$y = $&sigma;$(3*0 + 3*0 - 4) = $&sigma;$(-4) = \\frac{1}{1 + e^4} = 0.0180  $&asymp;$  0$\n",
        "\n",
        "If $x_{1}$ or $x_{2}$ (but not both) is equal to 1:\n",
        "\n",
        "$y = $&sigma;$(3*1 + 3*0 - 4) = $&sigma;$(-1) = \\frac{1}{1 + e} = 0.2689  $&asymp;$  0$\n",
        "\n",
        "If $x_{1}$ and $x_{2}$ are both 1:\n",
        "\n",
        "$y = $&sigma;$(3*1 + 3*1 - 4) = $&sigma;$(2) = \\frac{1}{1 + e^{-2}} = 0.8808  $&asymp;$  1$\n",
        "\n",
        "## Part B\n",
        "The problem here is a lack of linear separability.  With XOR, we have (0, 0) and (1,1) classified under label 0, and (0, 1) and (1, 0) classified under label 1.  Non-linear decision boundaries are not possible with a single-layer neural network.\n",
        "\n",
        "## Part C\n",
        "With a two-neuron hidden layer, each neuron can create a linear boundary--combined, these boundaries allow the network to produce the correct XOR output.\n",
        "\n",
        "## Part D\n",
        "The sigmoid function is a more effective activation function because of its continuity.  With the step function being discontinuous, it would prove problematic when training deep neural networks with gradient descent (since discontinuous functions are also non-differentiable).\n",
        "\n",
        "*Time spent: ~20 minutes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LgbYdt3wRag"
      },
      "source": [
        "# Problem 4\n",
        "## Part A\n",
        "Using Euclidean distance as a classifier with a 10,000-dimensional feature vector presents a high risk of encountering the curse of dimensionality--as the number of dimensions increases, the distances between points become increasingly uniform, resulting in a greater degree of ambiguity in classifying points based on their nearest neighbors.\n",
        "\n",
        "## Part B\n",
        "Principal component analysis (PCA) transforms the data such that dimensions are ordered by explained variance.  Removing lower-variance dimensions allows the user to mitigate the curse of dimensionality.  PCA is particularly useful in cases where a significant portion of the variance is explained by a small number of dimensions--for example, if the number of dimensions in the data can be reduced from 10,000 dimensions to 10 while retaining 95% of the variance, the accuracy of the kNN classifier would likely increase significantly.\n",
        "\n",
        "*Time spent: ~5 minutes*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 5\n",
        "## HTML Content"
      ],
      "metadata": {
        "id": "dnCoW2r8yRan"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6UgvtqewRag",
        "outputId": "5d23a6f6-ed19-465e-873d-ab400649c121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "c546a437-e044-44b7-a6db-30ce4d0fd54aAssignment 0In addition to responding to the questions on this assignment, please tell us approximately how much time you spent on each question. Thanks!Question 1. Bayes's Rule is often used in \"Bayesian updating\", since it is used to take a prior probability distribution and update it on the basis of observed evidence to produce a posterior distribution. Before, after, hence updating; get it?This notion of \"Bayesian updating\" is particularly useful when applied iteratively. Consider a scenario where we have a prior distribution P(H) over hypotheses, and then we observe evidence E1 and use Bayes's Rule to update our distribution toP(E1 | H) P(H)P(H | E1) =P(E1)The distribution P(H|E1) can now be viewed as a new, more informed prior. Therefore if we see a new piece of evidence E2, we can use Bayes's Rule again to derive a new posterior. Since P(H|E1) is the prior now, the result will take the formX P(H | E1) P(H | E1,E2) = ------------YDerive the simplest possible form of the full expression, i.e. fill in X and Y, assuming that E1 and E2 are independent. Please show your work. (Note that if you don't make that assumption, you won't get the right answer. Also recall that \"independent\" and \"conditionally independent given some variable\" are two different things.)Please don't forget to tell us approximately how much time you spent on this question.Question 2. First, go to Wolfram Alpha and try typing the following things into the box at the top (and then hit Enter or click the \"=\" sign). BinomialDistribution[10, 0.5] BinomialDistribution[100, 0.5]Fun, huh? Ok, now try it with the Beta distribution: BetaDistribution[1,1]  BetaDistribution[2,2] BetaDistribution[10,10]  BetaDistribution[2,10]  BetaDistribution[10,2]Note that in Wolfram Alpha, BetaDistribution[α,β] means the same beta distribution you'll see written elsewhere as Beta(α,β). Also note that this beta distribution should not be confused with the beta function written as B(α,β)!Look at the PDFs (probability density functions) for each one, and explain in plain English what each of these Beta distributions represents, when interpreted as a Bayesian prior for the probability of heads. (To keep things standard, let's all assume that the prior belief is about the probability of heads for apossibly-unfair coin.) What is your expectation about what the probability of heads will be, and how strongly do you think so?Let's take the notion of \"expectation\" in (4a) a little more literally, as in expected value, also known as the mean of the distribution. Try typing BetaDistribution[α,β] for some other values of α and β, and look at the number Wolfram Alpha reports as the \"mean\". Can you figure out what the formula for the mean is, as a function of α and β? (If you can't give an exact formula, at least comment on some properties of the mean. If you already knew the formula, it's ok to tell us so. But don't just look up the mean of the Beta distribution and copy it down; what would you learn from that?!)For refreshers on some of the material in this question, there's some nice content in Manning & Schuetze Section 2.1, on foundational concepts in probability theory, particularly the binomial distribution (Section 2.1.9) and the Bayesian updating example using coin flips (Section 2.1.10). (Christopher D. Manning and Hinrich Schuetze, Foundations of Statistical Natural Language Processing, published in 1999, is still a nice resource for many key concepts in NLP, even though the text we're using is more current. You can an online copy here.) Although not strictly necessary for this question this discussion of conjugate priors in Bayesian probability and particularly the example will make answering this question more worthwhile.Please don't forget to tell us approximately how much time you spent on this question.Question 3. Consider a fully connected feed-forward neural network with two layers: inputs (x1,x2) and output y, i.e. no hidden units. Assume that each unit behaves in the standard fashion by computing a weighted sum of its inputs and passing the result through a sigmoid activation function, using either an explicit bias or handling bias using a bias neuron in addition to the two inputs x1 and x2. Also assume that inputs x1 and x2 are binary values 0 or 1.Specify a network (either by giving all the parameters or drawing it, by hand is ok) that will output y = x1 AND x2 for all inputs.Briefly explain (with visual illustration if that helps) why you can't do the same thing for y = x1 XOR x2.Briefly explain (with visual illustration if that helps) how adding a hidden layer can solve the problem in (b).If you plot the sigmoid function y = 1/(1+exp (- 20x) ) on Wolfram Alpha, it looks an awful lot like the step function y = 0 for x < 0, y = 1 for x > 0, y undefined otherwise. Briefly but clearly explain why the former, not the latter, would be used as the activation function for units in deep neural networks.Please don't forget to tell us approximately how much time you spent on this question.Question 4. Consider a k-nearest-neighbors (k-NN) classifier. My goal is to use a training set (x,y) to classify new inputs x'.Suppose every input x is represented as a 10,000-dimensional feature vector. What might be some problems using k-NN with Euclidean distance as the classifier in this case?Very briefly explain what PCA is and how PCA could help with the problem in (a).Please note that answer this question does not require any equations. What we're asking about here is yourconceptual understanding, not the math. Explain things in plain English, the less technical the better. Please don't forget to tell us approximately how much time you spent on this question.Question 5. The programming part of this assignment is a simple exercise with text data. Here's what you should do, and please note that it's fine to be quick and dirty; the goal here is not to produce an elegant solution. You are welcome to do this entirely in Python, or using a combination of Python and Unix commands.Using whatever means you would like (e.g. wget, curl, viewing the page source HTML in a browser and copy/pasting, Python urllib, etc.), get the HTML source for this page.Use Beautiful Soup to extract the visible text from the page. (This discussion on StackOverflow might be helpful. Repeat after me: \"StackOverflow is my friend.\")Remove any character in the text that is not in [a-zA-Z] by turning it into whitespace.Convert the entire text to lowercase.Turn the text into \"words\" by splitting on whitespace.As your response, include:The 20 most frequent words in descending order, in the following format:150 the124 of115 and...etc. Your code. Note that if this requires more than one page of your homework PDF in a readable font, you almost certainly did more work than you needed to.Please don't forget to tell us approximately how much time you spent on this question.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('Assignment0-converted.html', 'r', encoding='utf-8') as myfile:\n",
        "  html_content = myfile.read()\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "text = soup.get_text()\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vBcR4wbcwRah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11050bda-23f4-4603-e2a9-67365b4c643c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 the\n",
            "29 a\n",
            "28 and\n",
            "27 in\n",
            "23 to\n",
            "22 you\n",
            "22 of\n",
            "19 this\n",
            "18 is\n",
            "17 x\n",
            "17 that\n",
            "16 e\n",
            "15 question\n",
            "14 on\n",
            "14 for\n",
            "12 as\n",
            "11 distribution\n",
            "10 it\n",
            "10 t\n",
            "9 please\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "pattern = '[^a-zA-Z]'\n",
        "\n",
        "cleaned_text = re.sub(pattern, ' ', text)\n",
        "\n",
        "cleaned_text = cleaned_text.lower()\n",
        "\n",
        "words = cleaned_text.split()\n",
        "\n",
        "word_counts = Counter(words)\n",
        "\n",
        "most_common_words = word_counts.most_common(20)\n",
        "\n",
        "for word, count in most_common_words:\n",
        "  print(f\"{count} {word}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}