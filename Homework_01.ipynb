{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ColeBromfield01/bromfield-portfolio/blob/main/Homework_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73Qw3cBdwRae"
      },
      "source": [
        "# Problem 1\n",
        "\n",
        "Just as $P(H | E_{1}) = \\frac{P(E_{1} | H)P(H)}{P(E_{1})}$:\n",
        "Our more informed posterior can take the form:\n",
        "\n",
        "$P(H | E_{1}, E_{2}) = \\frac{P(E_{2} | H, E_{1})P(H | E_{1})}{P(E_{2} | E_{1})}$\n",
        "\n",
        "Since $E_{1}$ and $E_{2}$ are independent,\n",
        "\n",
        "$P(E_{2}|H, E_{1}) = P(E_{2}|H)$\n",
        "\n",
        "and\n",
        "\n",
        "$P(E_{2}|E_{1}) = P(E_{2})$\n",
        "\n",
        "Therefore, our posterior can be rewritten as:\n",
        "\n",
        "$P(H | E_{1}, E_{2}) = \\frac{P(E_{2} | H)P(H | E_{1})}{P(E_{2})}$\n",
        "\n",
        "And thus, our X and Y values are:\n",
        "\n",
        "$X = P(E_{2}|H)$\n",
        "\n",
        "$Y = P(E_{2})$\n",
        "\n",
        "*Time spent: ~20 minutes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1T_I61WwRaf"
      },
      "source": [
        "# Problem 2\n",
        "## Part A\n",
        "### [1,1]\n",
        "With these parameters, we get a uniform distribution.  In a practical sense, we get an expected value for the probability of heads, but our confidence level is low, because all probabilities have equal likelihood.\n",
        "\n",
        "### [2,2]\n",
        "Once again, we get a symmetrical distribution, but no longer are all probabilities equally likely.  Now, more balanced values (closer to 50-50) have a higher likelihood, while fringe values (very high or very low probability of success) are less likely.\n",
        "\n",
        "### [10,10]\n",
        "The distribution is again centered, the strength of belief further increases.  Now, probabilities with any semblance of imbalance (outside of the 0.4-0.6 range) are highly unlikely.\n",
        "\n",
        "### [2,10]\n",
        "We now see a skewed distribution, with a low expected probability--probabilities above 0.5 approach a non-zero likelihood (suggesting a high strength of belief).\n",
        "\n",
        "### [10,2]\n",
        "This is simply the inverse of the [2,10] distribution--the expected probability is high, and probabilities below 0.5 are highly unlikely.\n",
        "\n",
        "## Part B\n",
        "I already knew the formula: divide alpha (the first parameter) by the sum of alpha and beta (the second parameter).  For example, for beta(10, 2), the expected probability is 10/(10+2) = 5/6.  Any pair of values with the same ratio will therefore produce the same expected probability, only differing in strength of belief (variance).\n",
        "\n",
        "*Time spent: ~ 15 minutes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6o3LaG0wRag"
      },
      "source": [
        "# Problem 3\n",
        "## Part A\n",
        "If we use weights of 3 and a bias term of -4, the output will be correct:\n",
        "* $y = $&sigma;$(3x_{1} + 3x_{2} - 4)$\n",
        "\n",
        "If $x_{1}$ and $x_{2}$ are both 0:\n",
        "\n",
        "$y = $&sigma;$(3*0 + 3*0 - 4) = $&sigma;$(-4) = \\frac{1}{1 + e^4} = 0.0180  $&asymp;$  0$\n",
        "\n",
        "If $x_{1}$ or $x_{2}$ (but not both) is equal to 1:\n",
        "\n",
        "$y = $&sigma;$(3*1 + 3*0 - 4) = $&sigma;$(-1) = \\frac{1}{1 + e} = 0.2689  $&asymp;$  0$\n",
        "\n",
        "If $x_{1}$ and $x_{2}$ are both 1:\n",
        "\n",
        "$y = $&sigma;$(3*1 + 3*1 - 4) = $&sigma;$(2) = \\frac{1}{1 + e^{-2}} = 0.8808  $&asymp;$  1$\n",
        "\n",
        "## Part B\n",
        "The problem here is a lack of linear separability.  With XOR, we have (0, 0) and (1,1) classified under label 0, and (0, 1) and (1, 0) classified under label 1.  Non-linear decision boundaries are not possible with a single-layer neural network.\n",
        "\n",
        "## Part C\n",
        "With a two-neuron hidden layer, each neuron can create a linear boundary--combined, these boundaries allow the network to produce the correct XOR output.\n",
        "\n",
        "## Part D\n",
        "The sigmoid function is a more effective activation function because of its continuity.  With the step function being discontinuous, it would prove problematic when training deep neural networks with gradient descent (since discontinuous functions are also non-differentiable).\n",
        "\n",
        "*Time spent: ~20 minutes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LgbYdt3wRag"
      },
      "source": [
        "# Problem 4\n",
        "## Part A\n",
        "Using Euclidean distance as a classifier with a 10,000-dimensional feature vector presents a high risk of encountering the curse of dimensionality--as the number of dimensions increases, the distances between points become increasingly uniform, resulting in a greater degree of ambiguity in classifying points based on their nearest neighbors.\n",
        "\n",
        "## Part B\n",
        "Principal component analysis (PCA) transforms the data such that dimensions are ordered by explained variance.  Removing lower-variance dimensions allows the user to mitigate the curse of dimensionality.  PCA is particularly useful in cases where a significant portion of the variance is explained by a small number of dimensions--for example, if the number of dimensions in the data can be reduced from 10,000 dimensions to 10 while retaining 95% of the variance, the accuracy of the kNN classifier would likely increase significantly.\n",
        "\n",
        "*Time spent: ~5 minutes*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 5"
      ],
      "metadata": {
        "id": "dnCoW2r8yRan"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6UgvtqewRag",
        "outputId": "935d5d48-a7a2-4416-b087-7e19ded91d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 the\n",
            "29 a\n",
            "28 and\n",
            "27 in\n",
            "23 to\n",
            "22 you\n",
            "22 of\n",
            "19 this\n",
            "18 is\n",
            "17 x\n",
            "17 that\n",
            "16 e\n",
            "15 question\n",
            "14 on\n",
            "14 for\n",
            "12 as\n",
            "11 distribution\n",
            "10 it\n",
            "10 t\n",
            "9 please\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Scraping HTML code from file\n",
        "with open('Assignment0-converted.html', 'r', encoding='utf-8') as myfile:\n",
        "  html_content = myfile.read()\n",
        "\n",
        "# Parsing text from HTML code\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "text = soup.get_text()\n",
        "\n",
        "# Replacing non-alphabetical characters with whitespace\n",
        "pattern = '[^a-zA-Z]'\n",
        "cleaned_text = re.sub(pattern, ' ', text)\n",
        "\n",
        "# Converting to lowercase\n",
        "cleaned_text = cleaned_text.lower()\n",
        "\n",
        "# Splitting into a list of words\n",
        "words = cleaned_text.split()\n",
        "\n",
        "# Getting the 20 most common words in the list\n",
        "word_counts = Counter(words)\n",
        "most_common_words = word_counts.most_common(20)\n",
        "\n",
        "for word, count in most_common_words:\n",
        "  print(f\"{count} {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Time spent: ~25 minutes*"
      ],
      "metadata": {
        "id": "SqMJMvv-3kbD"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}